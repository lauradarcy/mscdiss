\chapter{Problem Description}
In order to model a possible workflow, we need to model separate services in an environment. The best way to do this is to build an environment where mathematical operators act as the separate services, and the `ideal workflow' is a function hidden to the learning agent. At the start of each episode, the agent will be given an arbitrary number, and the agent can call one service per timestep in an attempt to reach the correct output value, calculated by the hidden function. The agent will receive a reward of 0 at each timestep until the episode either times out after 20 steps, or reaches the correct value, at which point it will receive a reward inversely proportional to the number of timesteps taken, i.e. $1/n$ where n is the number of operations used. This gives an environment that encourages the agent to to find the correct output in the smallest number of steps, so should learn to avoid circular paths, e.g $+1,-1,+1,-1$. It is important to note that in finding the `ideal' function, the agent may find a function that is different from the hidden function, yet returns the same values; the hidden function could be $x+1\times2$ (ignoring orders of operation, as each step is done separately), yet the agent may find $x\times2+2$, which although `different' does in fact return the same results in the same number of steps. For the purposes of this project, this is unimportant: so long as an `ideal workflow' is found, the specifics of the workflow itself don't matter.

	The most basic environment would include just four basic actions: $+1, -1, \times2$ and $\div2$. Adding additional functions, such as exponents, trigonometric functions, etc. would be trivial, yet add little actual value to the project itself, so only these four basic function types are examined. However, more detailed environments could have a range of discretised values: $+1, +1.1, +1.2...,+5.0$, for each action type, giving a large range of possible actions. This could potentially be expanded further to have multiple continuous action spaces, one for each operator type, with the value being continuous, however, this requires more advanced techniques to solve which are beyond the scope of this dissertation due to time constraint.

The states are very simple within the context of this environment. each state is simply the number of operations that have already been applied within the current episode. This is, however, non-Markov: without knowing the previous operations that have been applied to the input value, there is no way to know if the reward will be reached by performing any given action. Changing the state to include the previous operations will increase the number of states by a factor of at least $O(4^n)$ for each timestep for the most basic environment, more for environments with larger action spaces. Implementing the Markov property for this environment is therefore too computationally expensive to be considered worthwhile. This, combined with the action spaces that are unusually large for typical reinforcement learning problems (most deal with 4 to 5 actions at the most), makes for an environment much more difficult to solve than it may appear from face value. Both non-Markov and large action space environments have yet been fully explored within the Reinforcement learning field, yet are most analogous to real world distributed systems. 


**discussion of application to automated workflow, benefits.

**survey what has been done in field, wolpertinger etc.



